class KNNClassifier:
    def __init__(self, k=5):
        """ Initialize our custom KNN classifier
        Args:
            k: the number of nearest neighbors to consider for classification
        """
        self._k = k
        # the data structure for neighbor searching
        self._ball_tree = None
        #saves which training label 
        self._y = None
        # saves labels to an index
        self.label_to_index = None
        # then go backwards from index to get label
        self.index_to_label = None
        # saves most common label for tiebreaking
        self.training_most_common = None

    def fit(self, X, y):
        """ Fit the model using the provided data
        Args:
            X:  matrix of shape (num_training_samples, num_features)
            y: array of shape (num_training_samples,)
        Returns: Fit instance of KNNClassifier (self)
        """
        self._ball_tree = sklearn.neighbors.BallTree(X)  # See documentation of BallTree and how it's used
        self._y = y
        # Should be used to map the classes to {0,1,..C-1} if needed (C is the number of classes)
        # We can assume that the training data contains samples from all the possible classes
        classes = np.unique(y)
        self.label_to_index = dict(zip(classes, range(classes.shape[0])))
        self.index_to_label = dict(zip(range(classes.shape[0]), classes))
        
        label_values, label_counts = np.unique(y, return_counts=True)
        self.training_most_common = label_values[np.argmax(label_counts)]

        return self
    # creates a ball tree , stores labels , maps indexces ( forwards and back)

    def sample_label(self, index):
        # helper called in function fit
        # helper method to get label of sample index in majority_vote
        assert index < self._y.shape[0]
        return self._y[index]
        

    def majority_vote(self, indices_nearest_k, distances_nearest_k=None):
        """ Given indices of the nearest k neighbors for each point,
            report the majority label of those points.
        Args:
            k_nearest_indices: 2-d array of the indices of training neighbors, of shape (M, k)
            k_nearest_distances: 2-d array of the corresponding distances of training neighbors, of shape (M, k)
        Returns: The majority label for each row of indices, shape (M,)
        """

        # Workspace 1.2
        # TODO: Determine majority for each row of indices_nearest_k
        # TODO: if there is a tie, iteratively remove the last sample from the indices_nearest_k list until the tie is broken.
        voted_labels = np.empty(indices_nearest_k.shape[0])  # to include
        #BEGIN
        #YOUR CODE HERE
        for i in range(indices_nearest_k.shape[0]):
            #get the row by row over each neighbor
            row_indices = list(indices_nearest_k[i])
            labels = [self.sample_label(index) for index in row_indices]
            
            # save and count them
            label_counts = {}
            for label in labels:
                if label in label_counts:
                    label_counts[label] += 1
                else:
                    label_counts[label] = 1
            # now we need to see which is popular [label, number of times it shows up
            #unique_labels, label_counts = np.unique(neighbor_labels, return_counts=True)
            max_count = max(label_counts.values()) #.values is needed
            mode =  []
            for label, count in label_counts.items():
                if count == max_count:
                    mode.append(label) 

            #check for tie 
            while len(mode) > 1 and len(row_indices) > 0:
                if indices_nearest_k.shape[1] > 0:
                    row_indices.pop() 
                    label_counts = {}  
                    
                    for j in range(indices_nearest_k.shape[1]): 
                        neighbor_index = indices_nearest_k[i][j]
                        label = self.sample_label(neighbor_index)
                        if label in label_counts:
                            label_counts[label] += 1
                        else:
                            label_counts[label] = 1

                    max_count = max(label_counts.values())
                
                    mode =  []
                    for label, count in label_counts.items():
                        if count == max_count:
                            mode.append(label)
            voted_labels[i] = mode[0]
       
        #END
        return voted_labels

    def predict(self, X):
        """ Given new data points, classify them according to the training data
            provided in self.fit and number of neighbors self.k
            - You should use BallTree to get the distances and indices of
            the nearest k neighbors
        Args:
            X: feature vectors (num_samples, num_features)
        Returns:
            1-D array of predicted classes of shape (num_samples,)
        """
        # Workspace 1.1
        distances_nearest_k, indices_nearest_k = np.array([]), np.array([])  # changed :/
        #BEGIN
        distances_nearest_k, indices_nearest_k = self._ball_tree.query(X, self._k)
        #END
        return self.majority_vote(indices_nearest_k, distances_nearest_k)

    def confusion_matrix(self, X, y):
        """ Generate the confusion matrix for the given data
        Args:
            X: data matrix, shape (num_samples, num_features)
            y: the corresponding correct classes of our set, shape (num_samples,)
        Returns: a CxC matrix, where C is the number of classes in our training data
        """

        # The rows of the confusion matrix correspond to the counts from the true labels, the columns to the predictions'
        # Workspace 1.3
        # TODO: Run classification for the test set X, compare to test answers y, and add counts to matrix
        c_matrix = np.zeros((len(self.label_to_index), len(self.label_to_index)))
        #BEGIN
        # cant use this stuff but nice to look at 
        # show_decision_surface(model)
        # y_pred = model.predict(X_test)
        # conf = sklearn.metrics.confusion_matrix(y_test, y_pred)
        # disp = sklearn.metrics.ConfusionMatrixDisplay(conf)
        # disp.plot()
        # plt.show()
        predictions = self.predict(X)
        
        num_classes = len(self.label_to_index)
        
        #its a sqare bc binary
        matrix = np.zeros((num_classes, num_classes))
    
        for true, pred in zip(y, predictions):
            true_i = self.label_to_index[true]
            pred_i = self.label_to_index[pred]
            # fix indexing bc its dumb
            matrix[true_i, pred_i] += 1  
            #END
        return matrix

    def accuracy(self, X, y):
        """ Return the accuracy of the classifier on the data (X, y)
        Args:
            X: matrix of shape (num_samples, num_features)
            y: array of shape (num_samples,)

        Returns: accuracy score [float in (0,1)]
        """
        # Workspace 1.4
        # TODO: Compute accuracy on X
        # inc score = 0
        #BEGIN
        #YOUR CODE HERE
        score = 0
        #BEGIN
        predictions = self.predict(X)
        # get that aveerage 
        score = np.mean(predictions == y)
        #END
        #END
        return score
